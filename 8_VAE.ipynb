{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVUwxbe665ksd0L+cEhzWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donlap/stat424/blob/main/8_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZsC2z89sYii"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ› ï¸ Lab Setup\n",
        "!pip install equinox -q\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import equinox as eqx\n",
        "import optax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from ipywidgets import interact, FloatSlider, Layout\n",
        "\n",
        "# Global Configuration\n",
        "IMAGE_SIZE = 100\n",
        "LATENT_DIM = 10\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# Set random seed\n",
        "key = jr.PRNGKey(42)\n",
        "\n",
        "print(\"Libraries installed. configuration set to:\")\n",
        "print(f\"- Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
        "print(f\"- Latent Sliders: {LATENT_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸŽ¨ Load Data from GitHub\n",
        "REPO_URL = \"https://github.com/donlap/stat424\"\n",
        "FOLDER_NAME = \"faces\"\n",
        "\n",
        "def load_data_from_github(repo_url, folder_name=\"\"):\n",
        "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
        "\n",
        "    if os.path.exists(repo_name):\n",
        "        shutil.rmtree(repo_name)\n",
        "\n",
        "    ret = os.system(f\"git clone {repo_url}\")\n",
        "    if ret != 0:\n",
        "        print(\"âŒ Error cloning. Check URL.\")\n",
        "        return np.array([])\n",
        "\n",
        "    search_path = os.path.join(repo_name, folder_name, \"**\", \"*.png\")\n",
        "    files = glob(search_path, recursive=True)\n",
        "    print(f\"Found {len(files)} PNG files. Processing...\")\n",
        "\n",
        "    images = []\n",
        "    for f in files:\n",
        "        try:\n",
        "            img = Image.open(f).convert(\"RGB\")\n",
        "            img_arr = np.array(img) / 255.0\n",
        "            images.append(img_arr)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    shutil.rmtree(repo_name)\n",
        "\n",
        "    return np.array(images)\n",
        "\n",
        "data = load_data_from_github(REPO_URL, FOLDER_NAME)\n",
        "print(f\"âœ… Successfully loaded {len(data)} images of shape {data.shape[1:]}\")\n",
        "\n",
        "# Display samples\n",
        "plt.figure(figsize=(10, 2))\n",
        "for i in range(min(5, len(data))):\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.imshow(data[i])\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aV9qiYefsZQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸ§  Define VAE (Customized for 100x100)\n",
        "\n",
        "class Encoder(eqx.Module):\n",
        "    layers: list\n",
        "\n",
        "    def __init__(self, key):\n",
        "        keys = jr.split(key, 3)\n",
        "        self.layers = [\n",
        "            eqx.nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1, key=keys[0]),\n",
        "            eqx.nn.Lambda(jax.nn.relu),\n",
        "            eqx.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, key=keys[1]),\n",
        "            eqx.nn.Lambda(jax.nn.relu),\n",
        "        ]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x.flatten()\n",
        "\n",
        "class Decoder(eqx.Module):\n",
        "    linear: eqx.nn.Linear\n",
        "    layers: list\n",
        "\n",
        "    def __init__(self, latent_dim, key):\n",
        "        keys = jr.split(key, 3)\n",
        "        self.linear = eqx.nn.Linear(latent_dim, 64 * 25 * 25, key=keys[0])\n",
        "\n",
        "        self.layers = [\n",
        "            eqx.nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, key=keys[1]),\n",
        "            eqx.nn.Lambda(jax.nn.relu),\n",
        "            eqx.nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1, key=keys[2]),\n",
        "            # code here # Output [0, 1]\n",
        "        ]\n",
        "\n",
        "    def __call__(self, z):\n",
        "        x = self.linear(z)\n",
        "        x = x.reshape((64, 25, 25))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class VAE(eqx.Module):\n",
        "    encoder: Encoder\n",
        "    decoder: Decoder\n",
        "    fc_mu: eqx.nn.Linear\n",
        "    fc_logvar: eqx.nn.Linear\n",
        "\n",
        "    def __init__(self, latent_dim, key):\n",
        "        k1, k2, k3, k4 = jr.split(key, 4)\n",
        "        self.encoder = Encoder(k1)\n",
        "        self.decoder = Decoder(latent_dim, k2)\n",
        "\n",
        "        flat_size = 64 * 25 * 25\n",
        "        # code here\n",
        "\n",
        "\n",
        "    def __call__(self, x, key):\n",
        "        encoded = self.encoder(x)\n",
        "        # code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return self.decoder(z), mu, log_var\n",
        "\n",
        "# Initialize\n",
        "key, subkey = jr.split(key)\n",
        "model = VAE(latent_dim=10, key=subkey)\n",
        "print(f\"Model initialized with 3 Latent Dimensions for 100x100 images.\")"
      ],
      "metadata": {
        "id": "xaxx8rO85C72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸ‹ï¸â€â™€ï¸ Train\n",
        "# Loss function\n",
        "def loss_fn(model, x, key):\n",
        "    recon, mu, log_var = model(x, key)\n",
        "    # code here\n",
        "\n",
        "\n",
        "    return # code here\n",
        "\n",
        "loss_fn_batch = jax.vmap(loss_fn, in_axes=(None, 0, 0))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optax.adam(LEARNING_RATE)\n",
        "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
        "\n",
        "@eqx.filter_jit\n",
        "def make_step(model, opt_state, x, key):\n",
        "    keys = jr.split(key, x.shape[0])\n",
        "    loss_val, grads = eqx.filter_value_and_grad(\n",
        "        lambda m, x, k: jnp.mean(loss_fn_batch(m, x, k))\n",
        "    )(model, x, keys)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    return model, opt_state, loss_val\n",
        "\n",
        "# Training\n",
        "EPOCHS = 1000\n",
        "losses = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    key, subkey = jr.split(key)\n",
        "    perm = jr.permutation(subkey, len(data))\n",
        "    shuffled_data = data[perm]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    for i in range(0, len(data), BATCH_SIZE):\n",
        "        batch = shuffled_data[i : i + BATCH_SIZE]\n",
        "        # JAX expects (C, H, W), Data is (H, W, C)\n",
        "        batch = jnp.transpose(batch, (0, 3, 1, 2))\n",
        "\n",
        "        key, sk = jr.split(key)\n",
        "        model, opt_state, loss = make_step(model, opt_state, batch, sk)\n",
        "        epoch_loss += loss\n",
        "        steps += 1\n",
        "\n",
        "    avg_loss = epoch_loss / steps\n",
        "    losses.append(avg_loss)\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Loss {avg_loss:.2f}\")\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I0ulPlSSscjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸŽ›ï¸ The 3-Slider Explorer\n",
        "\n",
        "def generate_face(z0, z1, z2, z3, z4, z5, z6, z7, z8, z9):\n",
        "    # Construct the latent vector from the 3 sliders\n",
        "    z = jnp.array([z0, z1, z2, z3, z4, z5, z6, z7, z8, z9])\n",
        "\n",
        "    # Decode\n",
        "    img = model.decoder(z)\n",
        "\n",
        "    # Format for display (C, H, W) -> (H, W, C)\n",
        "    img = jnp.transpose(img, (1, 2, 0))\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Latent: [{z0:.1f}, {z1:.1f}, {z2:.1f}, {z3:.1f}, {z4:.1f}, {z5:.1f}, {z6:.1f}, {z7:.1f}, {z8:.1f}, {z9:.1f}]\")\n",
        "    plt.show()\n",
        "\n",
        "# Define the 3 specific sliders\n",
        "slider_layout = Layout(width='500px')\n",
        "z0_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 1', layout=slider_layout)\n",
        "z1_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 2', layout=slider_layout)\n",
        "z2_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 3', layout=slider_layout)\n",
        "z3_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 4', layout=slider_layout)\n",
        "z4_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 5', layout=slider_layout)\n",
        "z5_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 6', layout=slider_layout)\n",
        "z6_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 7', layout=slider_layout)\n",
        "z7_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 8', layout=slider_layout)\n",
        "z8_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 9', layout=slider_layout)\n",
        "z9_slider = FloatSlider(min=-3, max=3, step=0.1, value=0, description='Feature 10', layout=slider_layout)\n",
        "\n",
        "print(\"Interact with the 3 latent dimensions:\")\n",
        "interact(generate_face, z0=z0_slider, z1=z1_slider, z2=z2_slider, z3=z3_slider, z4=z4_slider, z5=z5_slider, z6=z6_slider, z7=z7_slider, z8=z8_slider, z9=z9_slider);"
      ],
      "metadata": {
        "id": "XvhbXV4qxw3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸ“Š Visualize Latent Distributions (Grid View)\n",
        "import math\n",
        "\n",
        "def visualize_latents(model, data, batch_size=64):\n",
        "    @eqx.filter_jit\n",
        "    def encode_batch(model, x):\n",
        "        def encode_single(img):\n",
        "            return model.fc_mu(model.encoder(img))\n",
        "        return jax.vmap(encode_single)(x)\n",
        "\n",
        "    print(\"Encoding all images to latent space...\")\n",
        "    mus = []\n",
        "\n",
        "    data_transposed = jnp.transpose(data, (0, 3, 1, 2))\n",
        "\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data_transposed[i : i + batch_size]\n",
        "        batch_mu = encode_batch(model, batch)\n",
        "        mus.append(batch_mu)\n",
        "\n",
        "    latent_vectors = np.concatenate(mus, axis=0)\n",
        "\n",
        "    num_latents = latent_vectors.shape[1]\n",
        "    cols = 5\n",
        "    rows = math.ceil(num_latents / cols)\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    print(f\"Plotting distributions for {num_latents} latent variables...\")\n",
        "\n",
        "    x_ref = np.linspace(-4, 4, 100)\n",
        "    y_ref = (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x_ref**2)\n",
        "\n",
        "    for i in range(num_latents):\n",
        "        ax = axes[i]\n",
        "\n",
        "        ax.hist(latent_vectors[:, i], bins=30, density=True, alpha=0.6, color='skyblue', label='Data')\n",
        "\n",
        "        ax.plot(x_ref, y_ref, 'r--', linewidth=2, label='Ideal Standard Normal')\n",
        "\n",
        "        ax.set_title(f\"Latent Dim {i}\")\n",
        "        ax.set_yticks([])\n",
        "        if i == 0:\n",
        "            ax.legend(fontsize=8)\n",
        "\n",
        "    for i in range(num_latents, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_latents(model, data)"
      ],
      "metadata": {
        "id": "CryAGgHapUMu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}